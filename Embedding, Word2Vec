# Embedding
임베딩 : 자연어를 벡터형태로 변환하는 것

단어나 문장을 '의미' 와 '연관성' 을 수치화 해서 벡터 공간으로 표현 → 딥러닝 모델의 입력값으로 사용하기 위함

임베딩 기법 2가지

1. 문장임베딩 : 문장 전체를 벡터로 표현
1. 단어임베딩 : 개별 단어를 벡터로 표현

# Word2Vec : 문장 내부의 단어를 벡터로 변환하는 도구
# 단어의 연결을 기반으로 단어의 연관성을 벡터로 만들어줌
# 즉, 단어를 벡터로 표현해줌.

# 단어를 벡터로 사용하면 , 단어의 '유사도' 를 쉽게 확인할수 있다.

# Word2Vec 을 구현하는 도구는 굉장히 많다.  이중에 실행속도도 빠르고 파이썬으로 실행할수 있는 Gensim 라이브러리 사용
# Gensim : 자연 언어 처리를 위한 라이브러리

# http://radimrehurek.com/gensim/

# > pip install gensim
# > conda install gensim

# > pip install python-Levenshtein   <-- Visual C++ Build Tool 없으면 설치 안되는 듯..
# > conda install python-Levenshtein


# Word2Vec 으로 '토지' 읽어오기

import codecs
from bs4 import BeautifulSoup
from konlpy.tag import Okt
from gensim.models import word2vec

# gensim.models.word2vec
# https://radimrehurek.com/gensim/models/word2vec.html
# 딥러닝을 통한 워드 벡터 생성
# Produce word vectors with deep learning via word2vec’s “skip-gram and CBOW models”, 
# using either hierarchical softmax or negative sampling 


filepath = r'D:\DevRoot\dataset\chatbot\BEXX0003.txt'
fp = codecs.open(filepath, "r", encoding="utf-16")
soup = BeautifulSoup(fp, "html.parser")
body = soup.select_one("body > text")
text = body.getText()
text  # 확인

#### 어미, 조사, 구두점 제거

- 명사, 동사, 형용사..만 학습시키기 위함
- 동사, 형용사 는 기본형으로만 학습

okt = Okt()
results = []
lines = text.split('\n')

for line in lines:
    malist = okt.pos(line, norm=True, stem=True)
    r = []
    for word in malist:
        # 어미/조사/구두점 등은 대상에서 제외
        if not word[1] in ["Josa", "Eomi", "Punctuation"]:
            r.append(word[0])
            
    r1 = (" ".join(r)).strip()
    results.append(r1)
    print(r1)

##### 위 결과를 파일로 출력

wakati_file = r'D:\DevRoot\dataset\chatbot\toji.wakati'
with open(wakati_file, 'w', encoding='utf-8') as fp:
    fp.write('\n'.join(results))

#### Word2Vec 모델 만들기

data = word2vec.LineSentence(wakati_file)
data

# 딥러닝을 통한 word vector 모델 학습

model = word2vec.Word2Vec(data,
                         vector_size=200,   # 벡터의 차원
                         window=10,    # 한 문장 내에서 단어간 최대 거리값
                         hs=1,        # hs=1   --> hierarchical softmax 학습
                         min_count=2,   # 등장빈도가 이보다 낮으면 무시
                         sg=1,        # 학습알고리즘 선택. sg=1  이면 skip-gram 사용
                         )

model

Word2Vec 은 단어 임베딩 '모델' 

1. skip-gram 모델 : 타깃 단어를 이용해서 주변 단어들을 예측하는 모델
1. CBOW (Coutinuous bag-of-words)모델 : 주변 단어들을 이용해 타깃 단어를 예측하는 모델

#### 윈도우 window 란?
타겟 단어를 예측하기 위해 앞뒤 단어를 확인하는 개수

![image.png](attachment:image.png)

#### 위에서 학습한 모델, 저장

save_path = r'D:\DevRoot\dataset\chatbot\toji.model'
model.save(save_path)
print('ok')

#### 저장된 모델 읽어오기

model = None
model = word2vec.Word2Vec.load(save_path)

# 학습된 말뭉치(corpus) 개수
model.corpus_count

# 말뭉치(corpus) 내 전체 단어 개수
model.corpus_total_words

#### 모델 확용

model.wv.most_similar(positive=["땅"])

# '토지' 라는 책에서 "땅" 과 가까운 단어로 아래와 같은 단어들이 나옴.
# ※머신러닝의 학습은 '랜덤' 에서 시작하기때문에 결과는 조금씩 다를수 있다.

model.wv['땅']

# vector_size=200  으로 학습한 결과
# (200, ) array 

model.wv.most_similar(positive=['집'])

model.wv.most_similar(positive=['집'], topn=3)  # 가장 유사한 단어 상위 3개

# 위키피디아 한국어 벼전을 사전으로 사용해보기

# 사실 소설 한두권으로는 데이터가 부족하다
# 조금 더 많은 데이터로 Word2Vec 을 테스트해봅시다.

# 위키피디아 (한국어) 내용을 사용하겠습니다.  용량이 꽤 크므로 시간이 꽤 걸리는 예제가 될겁니다.  (★10시간 넘을수도★)

# 위키피디아 (한국어판) 데이터 다운로드

#  https://dumps.wikimedia.org/kowiki/latest

# 위 페이지에서 kowiki-latest-pages-articles.xml.bz2  파일을 다운 받아서 압축을 풉니다.

model = word2vec.Word2Vec.load(r'D:\DevRoot\dataset\wiki\wiki.model')
model

print("corpus_count : ", model.corpus_count)
print("corpus_total_words : ", model.corpus_total_words)

# "Python", "파이썬" 의 유사어를 조사
model.wv.most_similar(positive=["Python", "파이썬"])

# 아빠 - 남성 + 여성
model.wv.most_similar(positive=['아빠', '여성'], negative=['남성'])

# 왕자 - 남성 + 여성
model.wv.most_similar(positive=['왕자', '여성'], negative=['남성'])

# 한국에서 서울에 해당하는 곳이 일본에서는 어디인가요?
model.wv.most_similar(positive=['서울', '일본'], negative=['한국'])

# 중국의 수도는 어디일까요
model.wv.most_similar(positive=['서울', '중국'], negative=['한국'])

# 오른쪽 + 남자 - 왼쪽

model.wv.most_similar(positive=["오른쪽", "남자"], negative=["왼쪽"])

# 서울에서 맛집으로 유명한 지역은?  서울 + 맛집

model.wv.most_similar(positive=["서울", "맛집"])

model.wv['고양이']

# red,...
# 붉은색, 적색, 빨강....



### 희소 표현과 분산 표현

##### 희소 표현 (sparse representation)
단어의 인덱스 요소만 1 이고, 나머지 요소는 모두 0 으로 표현되는 희소 벡터

![image.png](attachment:image.png)

- 희소 표현의 장점: 각각의 차원이 독립적인 정보를 갖고 있어서 사람이 이해하기에는 직관적-

- 희소 표현의 단점: 
    - 단어사전의 크기가 커질수록 메모리 낭비와 계산 복잡도 커짐.  
    - 단어간의 **연관성**이 전혀 없어 '의미'를 담을수 없다


##### 분산 표현 (distributed representation) 
한 단어의 정보가 특정 차원에 표현되지 않고 여러 차원에 분산되어 표현됨..   즉 하나의 차원에 다양한 정보를 가지고 있다.

ex: RGB 모델은 색상을 분산표현하는 방식  .  RGB(204, 255, 204) 형태로 분산 표현하는 것이다.  만약 이를 희소 벡터로 한다면 벡터 차원의 크기가 엄청날 것이다.  게다가 다른 색상과의 유사성 파악도 힘들다.

단어 임베딩에서도 분산표현 방식을 많이 사용합니다.


